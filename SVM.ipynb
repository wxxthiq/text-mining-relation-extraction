{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import json\n",
    "import torch\n",
    "import joblib\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import FunctionTransformer,MaxAbsScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Loading and Data Preprocessing of FewRel Dataset**\n",
    "This section retrieves and preprocesses the FewRel dataset, a benchmark for few-shot RE, chosen for its public availability, size (11,200 instances across 80 relations), and annotation reliability. \n",
    "\n",
    "The dataset is sourced by cloning the FewRel GitHub repository (`!git clone`), navigating to its directory (`%cd FewRel`), and loading `train_wiki.json` (8,400 instances) and `val_wiki.json` (2,800 instances) into memory as JSON dictionaries. Each instance contains tokenized sentences, head/tail entity details (names and token indices), and relation labels (e.g., \"P276\"). \n",
    "\n",
    "The `process_fewrel_json` function transforms these into a pandas DataFrame, extracting text, relations, entities, and indices (`h_seq`, `t_seq`). An adaptation is made by concatenating the training and validation sets into `df_merged` (11,200 instances), maximizing data for supervised learning—a departure from typical train/validation splits to leverage FewRel’s full corpus. The spaCy `en_core_web_sm` model is initialized for subsequent linguistic processing, setting the stage for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'FewRel' already exists and is not an empty directory.\n",
      "C:\\Users\\wathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wathi\\OneDrive\\Desktop\\Text Mining\\Assignment\\FewRel\n"
     ]
    }
   ],
   "source": [
    "# Clone FewRel repository from GitHub if not already present\n",
    "!git clone https://github.com/thunlp/FewRel\n",
    "# Change working directory to FewRel for file access\n",
    "%cd FewRel\n",
    "\n",
    "# Load training set (8,400 instances) from JSON\n",
    "with open('./data/train_wiki.json', 'r') as file:\n",
    "    fewrel_train = json.load(file)\n",
    "\n",
    "# Load validation set (2,800 instances) from JSON\n",
    "with open('./data/val_wiki.json', 'r') as file:\n",
    "    fewrel_val = json.load(file)\n",
    "\n",
    "def process_fewrel_json(fewrel_json):\n",
    "    \"\"\"Convert FewRel JSON data into a structured DataFrame.\"\"\"\n",
    "    texts, relations, heads, tails, h_seq, t_seq = [], [], [], [], [], []  # Initialize lists for DataFrame columns\n",
    "\n",
    "    for relation, instances in fewrel_json.items(): # Iterate over relation types\n",
    "        for instance in instances: # Process each instance within a relation\n",
    "            texts.append(' '.join(instance['tokens'])) # Join tokens into a full sentence\n",
    "            relations.append(relation) # Store relation ID (e.g., \"P276\")\n",
    "            heads.append(instance['h'][0]) # Extract head entity name\n",
    "            tails.append(instance['t'][0]) # Extract tail entity name\n",
    "            h_seq.append(instance['h'][2][0]) # Store head entity token indices\n",
    "            t_seq.append(instance['t'][2][0]) # Store tail entity token indices\n",
    "    # Return DataFrame with extracted features\n",
    "    return pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'relation': relations,\n",
    "        'head': heads,\n",
    "        'tail': tails,\n",
    "        'h_seq': h_seq,\n",
    "        't_seq': t_seq\n",
    "    })\n",
    "\n",
    "# Process train and validation datasets into DataFrames\n",
    "df_train = process_fewrel_json(fewrel_train)\n",
    "df_val = process_fewrel_json(fewrel_val)\n",
    "\n",
    "# Merge train and val sets (11,200 instances) to maximize supervised learning data\n",
    "df_merged = pd.concat([df_train, df_val], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Initialize spaCy model for linguistic feature extraction\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Feature Extraction**\n",
    "This section defines functions to extract a hybrid feature set, a key enhancement over traditional SVM-based RE, combining linguistic and semantic features from FewRel instances. \n",
    "\n",
    "The `extract_spacy_features` function uses spaCy to derive Named Entity Recognition (NER) labels (e.g., \"PERSON\"), Part-of-Speech (POS) tags (e.g., \"NOUN\"), and dependency relations (e.g., \"nsubj\") for head and tail entity spans, identified by `h_seq` and `t_seq`. These features capture structural context critical for relation classification.\n",
    "\n",
    "The `calculate_distance` function computes the token distance between entities, handling cases where entities are adjacent (distance = 0) or separated, encoding spatial relationships (e.g., for \"location\" relations). \n",
    " \n",
    "The `get_avg_vector` function generates semantic representations by averaging 100-dimensional GloVe embeddings (`glove-wiki-gigaword-100`) over entity tokens, with zero vectors for out-of-vocabulary words. \n",
    " \n",
    "This trio of functions forms the foundation of a custom feature extractor, blending traditional linguistic analysis with modern embeddings to improve SVM performance on FewRel’s diverse relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spacy_features(text, h_seq, t_seq):\n",
    "    \"\"\"Extract NER, POS, and dependency features for head and tail entities using spaCy.\"\"\"\n",
    "    \n",
    "    doc = nlp(text) # Process text with spaCy pipeline\n",
    "    \n",
    "    # Extract NER labels for head and tail spans (empty if no entities detected)\n",
    "    head_ner = [ent.label_ for ent in doc[h_seq[0]:h_seq[-1]+1].ents]\n",
    "    tail_ner = [ent.label_ for ent in doc[t_seq[0]:t_seq[-1]+1].ents]\n",
    "    \n",
    "    # Extract POS tags for head and tail spans\n",
    "    head_pos = [token.pos_ for token in doc[h_seq[0]:h_seq[-1]+1]]\n",
    "    tail_pos = [token.pos_ for token in doc[t_seq[0]:t_seq[-1]+1]]\n",
    "    \n",
    "    # Extract dependency relations for head and tail spans\n",
    "    head_dep = [token.dep_ for token in doc[h_seq[0]:h_seq[-1]+1]]\n",
    "    tail_dep = [token.dep_ for token in doc[t_seq[0]:t_seq[-1]+1]]\n",
    "    \n",
    "    return head_ner, tail_ner, head_pos, tail_pos, head_dep, tail_dep\n",
    "\n",
    "def calculate_distance(h_seq, t_seq):\n",
    "    \"\"\"Compute token distance between head and tail entities.\"\"\"\n",
    "    \n",
    "    if h_seq and t_seq: # Ensure both sequences are non-empty\n",
    "        head_end = max(h_seq) # End of head\n",
    "        tail_start = min(t_seq) # Start of tail\n",
    "        head_start = min(h_seq) # Start of head\n",
    "        tail_end = max(t_seq) # End of tail\n",
    "        \n",
    "        # Calculate distance based on entity positions\n",
    "        if head_end < tail_start:\n",
    "            distance = tail_start - head_end - 1 # Tokens between head and tail\n",
    "        elif tail_end < head_start:\n",
    "            distance = head_start - tail_end - 1 # Tokens between tail and head\n",
    "        else:\n",
    "            distance = 0 # Entities overlap or are adjacent\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def get_avg_vector(phrase, word_vectors):\n",
    "    \"\"\"Generate average GloVe embedding for a phrase.\"\"\"\n",
    "    \n",
    "    words = phrase.split() # Split phrase into tokens\n",
    "    # Fetch GloVe vectors, use zero vector for out-of-vocabulary words\n",
    "    vectors = [word_vectors[word] if word in word_vectors else np.zeros(word_vectors.vector_size) for word in words]\n",
    "    \n",
    "    # Return mean vector or zero vector if no valid embeddings\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(word_vectors.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Model Pipeline and Training**\n",
    "This section constructs and trains the SVM-based RE model using a scikit-learn `Pipeline`, integrating feature extraction, scaling, and classification.\n",
    "\n",
    "Pre-trained GloVe embeddings (`glove-wiki-gigaword-100`) are loaded to support semantic feature extraction. The `extract_features` function combines spaCy-derived features (NER/POS/dependency counts, distance) with GloVe embeddings (200D concatenated for head/tail), wrapped in a `CustomFeatureExtractor` transformer.\n",
    "\n",
    "Another novel adaptation, the `BertEmbedder` transformer, leverages `bert-base-uncased` to generate 768D mean-pooled sentence embeddings, enhancing contextual understanding beyond entity-specific features. \n",
    "\n",
    "These are unified via a `ColumnTransformer`, producing a 975D feature vector (207D custom + 768D BERT). The pipeline scales features with `MaxAbsScaler` to preserve sparsity and trains an SVM with an RBF kernel (`SVC(kernel='rbf', probability=True)`), chosen for its ability to model non-linear interactions among complex features.\n",
    "\n",
    "The merged dataset (`df_merged`) is split into 8,960 training and 2,240 testing instances (80-20, stratified by relation), and the pipeline is fitted to the training subset, optimizing for FewRel’s 80 relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;custom&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;extractor&#x27;,\n",
       "                                                                   CustomFeatureExtractor())]),\n",
       "                                                  [&#x27;text&#x27;, &#x27;h_seq&#x27;, &#x27;t_seq&#x27;,\n",
       "                                                   &#x27;head&#x27;, &#x27;tail&#x27;]),\n",
       "                                                 (&#x27;bert&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;bert&#x27;,\n",
       "                                                                   BertEmbedder())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;scaler&#x27;, MaxAbsScaler()),\n",
       "                (&#x27;svm&#x27;, SVC(probability=True, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;custom&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;extractor&#x27;,\n",
       "                                                                   CustomFeatureExtractor())]),\n",
       "                                                  [&#x27;text&#x27;, &#x27;h_seq&#x27;, &#x27;t_seq&#x27;,\n",
       "                                                   &#x27;head&#x27;, &#x27;tail&#x27;]),\n",
       "                                                 (&#x27;bert&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;bert&#x27;,\n",
       "                                                                   BertEmbedder())]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;scaler&#x27;, MaxAbsScaler()),\n",
       "                (&#x27;svm&#x27;, SVC(probability=True, random_state=42))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>features: ColumnTransformer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for features: ColumnTransformer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;custom&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;extractor&#x27;,\n",
       "                                                  CustomFeatureExtractor())]),\n",
       "                                 [&#x27;text&#x27;, &#x27;h_seq&#x27;, &#x27;t_seq&#x27;, &#x27;head&#x27;, &#x27;tail&#x27;]),\n",
       "                                (&#x27;bert&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;bert&#x27;, BertEmbedder())]),\n",
       "                                 &#x27;text&#x27;)])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>custom</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;text&#x27;, &#x27;h_seq&#x27;, &#x27;t_seq&#x27;, &#x27;head&#x27;, &#x27;tail&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>CustomFeatureExtractor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>CustomFeatureExtractor()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>bert</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>text</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>BertEmbedder</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>BertEmbedder()</pre></div> </div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MaxAbsScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.MaxAbsScaler.html\">?<span>Documentation for MaxAbsScaler</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>MaxAbsScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(probability=True, random_state=42)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 ColumnTransformer(transformers=[('custom',\n",
       "                                                  Pipeline(steps=[('extractor',\n",
       "                                                                   CustomFeatureExtractor())]),\n",
       "                                                  ['text', 'h_seq', 't_seq',\n",
       "                                                   'head', 'tail']),\n",
       "                                                 ('bert',\n",
       "                                                  Pipeline(steps=[('bert',\n",
       "                                                                   BertEmbedder())]),\n",
       "                                                  'text')])),\n",
       "                ('scaler', MaxAbsScaler()),\n",
       "                ('svm', SVC(probability=True, random_state=42))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained GloVe embeddings (100D) for semantic feature extractio\n",
    "word_vectors = api.load('glove-wiki-gigaword-100')\n",
    "\n",
    "def extract_features(df):\n",
    "    \"\"\"Extract hybrid feature set combining linguistic and GloVe embeddings.\"\"\"\n",
    "    \n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract linguistic features using spaCy\n",
    "        head_ner, tail_ner, head_pos, tail_pos, head_dep, tail_dep = extract_spacy_features(\n",
    "            row['text'], row['h_seq'], row['t_seq']\n",
    "        )        \n",
    "        \n",
    "        distance = calculate_distance(row['h_seq'], row['t_seq']) # Compute entity distance\n",
    "        \n",
    "        # Generate GloVe embeddings for head and tail entities\n",
    "        head_vector = get_avg_vector(row['head'], word_vectors)\n",
    "        tail_vector = get_avg_vector(row['tail'], word_vectors)\n",
    "        combined_vector = np.concatenate([head_vector, tail_vector]) # Concatenate to 200D\n",
    "        \n",
    "        # Combine all features into a 207D vector\n",
    "        feature_vector = np.concatenate([\n",
    "            np.array([len(head_ner), len(tail_ner)]),       # NER counts\n",
    "            np.array([len(head_pos), len(tail_pos)]),       # POS counts\n",
    "            np.array([len(head_dep), len(tail_dep)]),       # Dependency counts\n",
    "            np.array([distance]),                           # Distance feature\n",
    "            combined_vector                                 # Word embeddings (from GloVe)\n",
    "        ])\n",
    "        \n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.vstack(features) # Stack into a feature matrix\n",
    "\n",
    "class CustomFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer for hybrid feature extraction.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # No fitting required\n",
    "\n",
    "    def transform(self, X):\n",
    "        return extract_features(X) # Apply feature extraction to DataFrame\n",
    "\n",
    "# Define a BERT embedder transformer\n",
    "class BertEmbedder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer to generate BERT embeddings from text.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', device='cuda', pooling='mean'):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.pooling = pooling\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval() # Set to evaluation mode\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # No fitting required\n",
    "\n",
    "    def transform(self, X):\n",
    "        embeddings = []\n",
    "        with torch.no_grad(): # Disable gradient computation for inference\n",
    "            for text in X:\n",
    "                # Tokenize and encode text for BERT (max 512 tokens)\n",
    "                encoded_input = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "                encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}\n",
    "                output = self.model(**encoded_input)\n",
    "                \n",
    "                # Apply mean pooling to get 768D sentence embedding\n",
    "                if self.pooling == 'cls':\n",
    "                    emb = output.last_hidden_state[:, 0, :]\n",
    "                elif self.pooling == 'mean':\n",
    "                    emb = output.last_hidden_state.mean(dim=1)\n",
    "                else:\n",
    "                    raise ValueError(\"Pooling must be either 'cls' or 'mean'\")\n",
    "\n",
    "                embeddings.append(emb.squeeze().cpu().numpy())\n",
    "        \n",
    "        return np.vstack(embeddings) # Stack into embedding matrix\n",
    "\n",
    "# Define pipelines for custom features and BERT embeddings\n",
    "custom_features_pipeline = Pipeline([\n",
    "    ('extractor', CustomFeatureExtractor())\n",
    "])\n",
    "bert_pipeline = Pipeline([\n",
    "    ('bert', BertEmbedder(model_name='bert-base-uncased', device='cuda', pooling='mean'))\n",
    "])\n",
    "\n",
    "# Combine features using ColumnTransformer (975D total: 207D custom + 768D BERT)\n",
    "combined_features = ColumnTransformer([\n",
    "    ('custom', custom_features_pipeline, ['text', 'h_seq', 't_seq', 'head', 'tail']),\n",
    "    ('bert', bert_pipeline, 'text')\n",
    "])\n",
    "\n",
    "# Build final pipeline with scaling and SVM classifier\n",
    "pipeline = Pipeline([\n",
    "    ('features', combined_features), # Extract and combine features\n",
    "    ('scaler', MaxAbsScaler()), # Scale features, preserving sparsity\n",
    "    ('svm', SVC(kernel='rbf',probability=True, random_state=42)) # Train RBF SVM\n",
    "])\n",
    "\n",
    "# Prepare data for training\n",
    "X = df_merged  \n",
    "y = df_merged['relation']\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the pipeline to training data\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to 'svm_re_pipeline.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Export trained pipeline to file for reuse\n",
    "joblib.dump(pipeline, 'svm_re_pipeline.pkl')\n",
    "print(\"Model exported to 'svm_re_pipeline.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Model Evaluation**\n",
    "\n",
    "This section assesses the trained SVM pipeline’s performance on the test subset (2,240 instances). Predictions are generated using `pipeline.predict(X_test)`, and a `classification_report` computes precision, recall, and F1-score across 80 relations. \n",
    "\n",
    "The macro averaged F1-score of 0.78 reflects robust generalization, with per-relation metrics highlighting strengths (e.g., P105: 0.99 F1) and challenges (e.g., P40: 0.19 F1). This evaluation validates the efficacy of the hybrid feature set and RBF kernel, providing a quantitative basis for comparing this approach to the second RE method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       P1001       0.68      0.90      0.78       140\n",
      "        P101       0.78      0.75      0.77       140\n",
      "        P102       0.89      0.97      0.93       140\n",
      "        P105       0.98      1.00      0.99       140\n",
      "        P106       0.94      0.80      0.86       140\n",
      "        P118       0.91      0.98      0.94       140\n",
      "        P123       0.73      0.63      0.67       140\n",
      "        P127       0.59      0.49      0.53       140\n",
      "       P1303       0.91      0.97      0.94       140\n",
      "        P131       0.64      0.61      0.62       140\n",
      "       P1344       0.94      0.96      0.95       140\n",
      "       P1346       0.80      0.84      0.82       140\n",
      "        P135       0.82      0.92      0.87       140\n",
      "        P136       0.88      0.71      0.79       140\n",
      "        P137       0.61      0.75      0.68       140\n",
      "        P140       0.91      0.96      0.93       140\n",
      "       P1408       0.91      0.99      0.95       140\n",
      "       P1411       0.95      1.00      0.98       140\n",
      "       P1435       0.95      1.00      0.98       140\n",
      "        P150       0.89      0.83      0.86       140\n",
      "        P155       0.44      0.36      0.40       140\n",
      "        P156       0.49      0.26      0.34       140\n",
      "        P159       0.59      0.52      0.55       140\n",
      "         P17       0.81      0.77      0.79       140\n",
      "        P175       0.59      0.83      0.69       140\n",
      "        P176       0.74      0.89      0.81       140\n",
      "        P177       0.93      0.99      0.96       140\n",
      "        P178       0.60      0.81      0.69       140\n",
      "       P1877       0.68      0.74      0.71       140\n",
      "       P1923       0.79      0.95      0.86       140\n",
      "        P206       0.82      0.87      0.85       140\n",
      "       P2094       0.99      0.99      0.99       140\n",
      "         P22       0.35      0.35      0.35       140\n",
      "        P241       0.88      0.97      0.92       140\n",
      "         P25       0.47      0.71      0.57       140\n",
      "         P26       0.47      0.49      0.48       140\n",
      "        P264       0.82      0.94      0.88       140\n",
      "         P27       0.84      0.85      0.84       140\n",
      "        P276       0.64      0.63      0.64       140\n",
      "        P306       0.85      0.93      0.89       140\n",
      "         P31       0.76      0.39      0.51       140\n",
      "       P3373       0.56      0.49      0.52       140\n",
      "       P3450       0.96      1.00      0.98       140\n",
      "        P355       0.73      0.70      0.72       140\n",
      "        P361       0.50      0.19      0.28       140\n",
      "        P364       0.93      0.91      0.92       140\n",
      "         P39       0.87      0.90      0.88       140\n",
      "         P40       0.27      0.14      0.19       140\n",
      "        P400       0.85      0.91      0.88       140\n",
      "        P403       0.70      0.64      0.67       140\n",
      "        P407       0.78      0.85      0.82       140\n",
      "        P410       0.96      0.99      0.98       140\n",
      "        P412       1.00      0.99      1.00       140\n",
      "        P413       0.99      1.00      0.99       140\n",
      "        P449       0.84      0.96      0.90       140\n",
      "       P4552       0.86      0.99      0.92       140\n",
      "        P460       0.76      0.69      0.72       140\n",
      "        P463       0.82      0.74      0.78       140\n",
      "        P466       0.79      0.91      0.85       140\n",
      "        P495       0.86      0.77      0.81       140\n",
      "        P527       0.80      0.48      0.60       140\n",
      "        P551       0.64      0.57      0.60       140\n",
      "         P57       0.62      0.74      0.68       140\n",
      "         P58       0.62      0.54      0.57       140\n",
      "         P59       0.97      0.99      0.98       140\n",
      "          P6       0.94      0.96      0.95       140\n",
      "        P641       0.92      0.95      0.94       140\n",
      "        P674       0.67      0.82      0.74       140\n",
      "        P706       0.67      0.66      0.67       140\n",
      "        P710       0.75      0.71      0.73       140\n",
      "        P740       0.85      0.76      0.80       140\n",
      "        P750       0.87      0.85      0.86       140\n",
      "        P800       0.82      0.80      0.81       140\n",
      "         P84       0.95      0.96      0.95       140\n",
      "         P86       0.82      0.86      0.84       140\n",
      "        P921       0.81      0.66      0.73       140\n",
      "        P931       0.90      0.96      0.93       140\n",
      "        P937       0.76      0.72      0.74       140\n",
      "        P974       0.70      0.75      0.73       140\n",
      "        P991       0.96      0.96      0.96       140\n",
      "\n",
      "    accuracy                           0.78     11200\n",
      "   macro avg       0.78      0.78      0.78     11200\n",
      "weighted avg       0.78      0.78      0.78     11200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Inference Mode**\n",
    "This section prepares the inference environment and implements real-time RE by loading the exported model (`svm_re_pipeline.pkl`) with `joblib` and `pid2name.json` for relation metadata.\n",
    "\n",
    "If the FewRel repository was not cloned in Section 2, Uncomment and execute the next cell to clone the repository (`!git clone`) and navigate to its directory (`%cd FewRel`) to access `pid2name.json`. This file maps relation IDs to names and descriptions, enriching inference outputs. This step is optional if Section 2 was executed, ensuring flexibility for standalone inference runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wathi\\OneDrive\\Desktop\\Text Mining\\Assignment\\FewRel\\FewRel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'FewRel' already exists and is not an empty directory.\n",
      "C:\\Users\\wathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# Clone FewRel repository if not already present (optional if run earlier)\n",
    "#!git clone https://github.com/thunlp/FewRel\n",
    "\n",
    "# Navigate to FewRel directory to access pid2name.json\n",
    "#%cd FewRel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference mode relies on feature extraction utilities (`extract_spacy_features`, `calculate_distance`, `get_avg_vector`) and transformer classes (`CustomFeatureExtractor`, `BertEmbedder`), originally defined in Section 4. If those cells were not executed, the subsequent following cell must be run to define these components, ensuring the pipeline deserializes correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spacy_features(text, h_seq, t_seq):\n",
    "    \"\"\"Extract NER, POS, and dependency features for head and tail entities using spaCy.\"\"\"\n",
    "    \n",
    "    doc = nlp(text) # Process text with spaCy pipeline\n",
    "    \n",
    "    # Extract NER labels for head and tail spans (empty if no entities detected)\n",
    "    head_ner = [ent.label_ for ent in doc[h_seq[0]:h_seq[-1]+1].ents]\n",
    "    tail_ner = [ent.label_ for ent in doc[t_seq[0]:t_seq[-1]+1].ents]\n",
    "    \n",
    "    # Extract POS tags for head and tail spans\n",
    "    head_pos = [token.pos_ for token in doc[h_seq[0]:h_seq[-1]+1]]\n",
    "    tail_pos = [token.pos_ for token in doc[t_seq[0]:t_seq[-1]+1]]\n",
    "    \n",
    "    # Extract dependency relations for head and tail spans\n",
    "    head_dep = [token.dep_ for token in doc[h_seq[0]:h_seq[-1]+1]]\n",
    "    tail_dep = [token.dep_ for token in doc[t_seq[0]:t_seq[-1]+1]]\n",
    "    \n",
    "    return head_ner, tail_ner, head_pos, tail_pos, head_dep, tail_dep\n",
    "\n",
    "def calculate_distance(h_seq, t_seq):\n",
    "    \"\"\"Compute token distance between head and tail entities.\"\"\"\n",
    "    \n",
    "    if h_seq and t_seq: # Ensure both sequences are non-empty\n",
    "        head_end = max(h_seq) # End of head\n",
    "        tail_start = min(t_seq) # Start of tail\n",
    "        head_start = min(h_seq) # Start of head\n",
    "        tail_end = max(t_seq) # End of tail\n",
    "        \n",
    "        # Calculate distance based on entity positions\n",
    "        if head_end < tail_start:\n",
    "            distance = tail_start - head_end - 1 # Tokens between head and tail\n",
    "        elif tail_end < head_start:\n",
    "            distance = head_start - tail_end - 1 # Tokens between tail and head\n",
    "        else:\n",
    "            distance = 0 # Entities overlap or are adjacent\n",
    "\n",
    "    return distance\n",
    "\n",
    "def get_avg_vector(phrase, word_vectors):\n",
    "    \"\"\"Generate average GloVe embedding for a phrase.\"\"\"\n",
    "    \n",
    "    words = phrase.split() # Split phrase into tokens\n",
    "    # Fetch GloVe vectors, use zero vector for out-of-vocabulary words\n",
    "    vectors = [word_vectors[word] if word in word_vectors else np.zeros(word_vectors.vector_size) for word in words]\n",
    "    \n",
    "    # Return mean vector or zero vector if no valid embeddings\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(word_vectors.vector_size)\n",
    "\n",
    "# Initialize spaCy model (needed for extract_relation)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load pre-trained GloVe embeddings (100D) for semantic feature extractio\n",
    "word_vectors = api.load('glove-wiki-gigaword-100')\n",
    "\n",
    "def extract_features(df):\n",
    "    \"\"\"Extract hybrid feature set combining linguistic and GloVe embeddings.\"\"\"\n",
    "    \n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract linguistic features using spaCy\n",
    "        head_ner, tail_ner, head_pos, tail_pos, head_dep, tail_dep = extract_spacy_features(\n",
    "            row['text'], row['h_seq'], row['t_seq']\n",
    "        )        \n",
    "        \n",
    "        distance = calculate_distance(row['h_seq'], row['t_seq']) # Compute entity distance\n",
    "        \n",
    "        # Generate GloVe embeddings for head and tail entities\n",
    "        head_vector = get_avg_vector(row['head'], word_vectors)\n",
    "        tail_vector = get_avg_vector(row['tail'], word_vectors)\n",
    "        combined_vector = np.concatenate([head_vector, tail_vector]) # Concatenate to 200D\n",
    "        \n",
    "        # Combine all features into a 207D vector\n",
    "        feature_vector = np.concatenate([\n",
    "            np.array([len(head_ner), len(tail_ner)]),       # NER counts\n",
    "            np.array([len(head_pos), len(tail_pos)]),       # POS counts\n",
    "            np.array([len(head_dep), len(tail_dep)]),       # Dependency counts\n",
    "            np.array([distance]),                           # Distance feature\n",
    "            combined_vector                                 # Word embeddings (from GloVe)\n",
    "        ])\n",
    "        \n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.vstack(features) # Stack into a feature matrix\n",
    "\n",
    "class CustomFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer for hybrid feature extraction.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # No fitting required\n",
    "\n",
    "    def transform(self, X):\n",
    "        return extract_features(X) # Apply feature extraction to DataFrame\n",
    "\n",
    "# Define a BERT embedder transformer\n",
    "class BertEmbedder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer to generate BERT embeddings from text.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', device='cuda', pooling='mean'):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.pooling = pooling\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval() # Set to evaluation mode\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # No fitting required\n",
    "\n",
    "    def transform(self, X):\n",
    "        embeddings = []\n",
    "        with torch.no_grad(): # Disable gradient computation for inference\n",
    "            for text in X:\n",
    "                # Tokenize and encode text for BERT (max 512 tokens)\n",
    "                encoded_input = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "                encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}\n",
    "                output = self.model(**encoded_input)\n",
    "                \n",
    "                # Apply mean pooling to get 768D sentence embedding\n",
    "                if self.pooling == 'cls':\n",
    "                    emb = output.last_hidden_state[:, 0, :]\n",
    "                elif self.pooling == 'mean':\n",
    "                    emb = output.last_hidden_state.mean(dim=1)\n",
    "                else:\n",
    "                    raise ValueError(\"Pooling must be either 'cls' or 'mean'\")\n",
    "\n",
    "                embeddings.append(emb.squeeze().cpu().numpy())\n",
    "        \n",
    "        return np.vstack(embeddings) # Stack into embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not done so yet, Upload the model file 'svm_re_pipeline.pkl' into the FewRel Directory.\n",
    "\n",
    "The `extract_relation` function processes a user-supplied sentence and entities, using spaCy to tokenize and locate entity indices (`h_seq`, `t_seq`). It constructs a DataFrame, predicts the relation and probability via the loaded pipeline, and outputs the result with descriptive text.\n",
    "\n",
    "An example demonstrates this with \"London Heathrow Airport serves the greater London area efficiently.\" (head: \"London Heathrow Airport\", tail: \"London\"), correctly predicting \"P931\" with a probability of 0.48."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from 'svm_re_pipeline.pkl'\n",
      "The relation of 'London Heathrow Airport' and 'London' in the sentence\n",
      "'London Heathrow Airport serves London daily.' is 'P931'\n",
      "- Relation name: place served by transport hub\n",
      "- Description: territorial entity or entities served by this transport hub (airport, train station, etc.)\n",
      "- Probability of relation: 0.54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the trained SVM model\n",
    "loaded_pipeline = joblib.load('svm_re_pipeline.pkl')\n",
    "print(\"Model loaded from 'svm_re_pipeline.pkl'\")\n",
    "\n",
    "# Load relation metadata for descriptive outputs\n",
    "with open('./data/pid2name.json', 'r') as file:\n",
    "    pid2name = json.load(file)\n",
    "    \n",
    "def extract_relation(sentence, head, tail):\n",
    "    \"\"\"Predict relation between head and tail entities in a sentence.\"\"\"\n",
    "    \n",
    "    doc = nlp(sentence)# Tokenize sentence with spaCy\n",
    "    tokens = [token.text for token in doc] # Convert to token list\n",
    "    \n",
    "    # Find consecutive tokens matching 'head' and 'tail'\n",
    "    def find_token_indices(entity_text):\n",
    "        \"\"\"Locate token indices for an entity in the sentence.\"\"\"\n",
    "        \n",
    "        entity_words = entity_text.lower().split() # Split entity into words\n",
    "        n = len(entity_words)\n",
    "        tokens_lower = [t.lower() for t in tokens] # Lowercase tokens\n",
    "        \n",
    "        for i in range(len(tokens_lower) - n + 1): # Search for consecutive match\n",
    "            if tokens_lower[i:i+n] == entity_words:\n",
    "                return list(range(i, i+n)) # Return token indices\n",
    "        return [] # Return empty list if not found\n",
    "    \n",
    "    # Find token indices for head and tail entities\n",
    "    h_seq = find_token_indices(head)\n",
    "    t_seq = find_token_indices(tail)\n",
    "    \n",
    "    # If we couldn't find a match for either entity, we can't proceed\n",
    "    if not h_seq or not t_seq:\n",
    "        print(\"Could not locate head or tail entity in the tokenized sentence. Please ensure exact match.\")\n",
    "        return\n",
    "    \n",
    "    # Create input DataFrame for pipeline prediction\n",
    "    df_input = pd.DataFrame({\n",
    "        'text': [sentence],\n",
    "        'head': [head],\n",
    "        'tail': [tail],\n",
    "        'h_seq': [h_seq],\n",
    "        't_seq': [t_seq]\n",
    "    })\n",
    "    \n",
    "    # Predict relation and probability\n",
    "    pred_rel = loaded_pipeline.predict(df_input)[0]\n",
    "    proba = loaded_pipeline.predict_proba(df_input)[0]\n",
    "    classes = loaded_pipeline.named_steps['svm'].classes_\n",
    "    rel_idx = list(classes).index(pred_rel)\n",
    "    rel_prob = proba[rel_idx]\n",
    "    \n",
    "    # Attempt to retrieve relation info from pid2name (which has [name, description])\n",
    "    if pred_rel in pid2name:\n",
    "        rel_data = pid2name[pred_rel]  # e.g. [\"place served by transport hub\", \"...\"]\n",
    "        # Unpack the two-element list\n",
    "        if len(rel_data) == 2:\n",
    "            rel_name, rel_desc = rel_data\n",
    "        else:\n",
    "            # If for some reason the list isn't exactly length 2\n",
    "            rel_name = rel_data[0] if len(rel_data) > 0 else \"Unknown relation name\"\n",
    "            rel_desc = rel_data[1] if len(rel_data) > 1 else \"No description available.\"\n",
    "    else:\n",
    "        rel_name = \"Unknown relation name\"\n",
    "        rel_desc = \"No description available.\"\n",
    "    \n",
    "    # Display prediction results\n",
    "    print(f\"The relation of '{head}' and '{tail}' in the sentence\")\n",
    "    print(f\"'{sentence}' is '{pred_rel}'\")\n",
    "    print(f\"- Relation name: {rel_name}\")\n",
    "    print(f\"- Description: {rel_desc}\")\n",
    "    print(f\"- Probability of relation: {rel_prob:.2f}\\n\")\n",
    "\n",
    "# Prompt user for input\n",
    "# Static example for submission\n",
    "sentence = \"London Heathrow Airport serves London daily.\"\n",
    "head = \"London Heathrow Airport\"\n",
    "tail = \"London\"\n",
    "\n",
    "extract_relation(sentence, head, tail)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
